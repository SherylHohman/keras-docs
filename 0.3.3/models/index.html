<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Models - Keras Documentation</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Models";
    var mkdocs_page_input_path = "models.md";
    var mkdocs_page_url = "/models/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-61785484-1', 'keras.io');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Keras Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../documentation/">Index</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../examples/">Examples</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../faq/">FAQ</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../backend/">Backends</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../optimizers/">Optimizers</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../objectives/">Objectives</a>
	    </li>
          
            <li class="toctree-l1 current">
		
    <a class="current" href="./">Models</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#using-the-sequential-model">Using the Sequential model</a></li>
    

    <li class="toctree-l2"><a href="#using-the-graph-model">Using the Graph model</a></li>
    

    <li class="toctree-l2"><a href="#model-api-documentation">Model API documentation</a></li>
    

    <li class="toctree-l2"><a href="#sequential">Sequential</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#methods">Methods</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#graph">Graph</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#methods_1">Methods</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#model">Model</a></li>
    

    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../activations/">Activations</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../initializations/">Initializations</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../regularizers/">Regularizers</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../constraints/">Constraints</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../callbacks/">Callbacks</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../datasets/">Datasets</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../visualization/">Visualization</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Layers</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../layers/core/">Core Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/convolutional/">Convolutional Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/recurrent/">Recurrent Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/advanced_activations/">Advanced Activations Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/normalization/">Normalization Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/embeddings/">Embedding Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/noise/">Noise layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/containers/">Containers</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Preprocessing</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../preprocessing/sequence/">Sequence Preprocessing</a>
                </li>
                <li class="">
                    
    <a class="" href="../preprocessing/text/">Text Preprocessing</a>
                </li>
                <li class="">
                    
    <a class="" href="../preprocessing/image/">Image Preprocessing</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Keras Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Models</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="http://github.com/fchollet/keras/edit/master/docs/models.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p>Keras has two models: <strong>Sequential</strong>, a linear stack of layers, and <strong>Graph</strong>, a directed acyclic graph of layers.</p>
<h1 id="using-the-sequential-model">Using the Sequential model</h1>
<pre><code class="python">from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.optimizers import SGD

model = Sequential()
model.add(Dense(2, init='uniform', input_dim=64))
model.add(Activation('softmax'))

model.compile(optimizer='sgd', loss='mse')

'''
Train the model for 3 epochs, in batches of 16 samples,
on data stored in the Numpy array X_train,
and labels stored in the Numpy array y_train:
'''
model.fit(X_train, y_train, nb_epoch=3, batch_size=16, verbose=1)
'''
What you will see with mode verbose=1:
Train on 37800 samples, validate on 4200 samples
Epoch 0
37800/37800 [==============================] - 7s - loss: 0.0385
Epoch 1
37800/37800 [==============================] - 8s - loss: 0.0140
Epoch 2
10960/37800 [=======&gt;......................] - ETA: 4s - loss: 0.0109
'''

model.fit(X_train, y_train, nb_epoch=3, batch_size=16, verbose=2)
'''
What you will see with mode verbose=2:
Train on 37800 samples, validate on 4200 samples
Epoch 0
loss: 0.0190
Epoch 1
loss: 0.0146
Epoch 2
loss: 0.0049
'''

'''
Demonstration of the show_accuracy argument
'''
model.fit(X_train, y_train, nb_epoch=3, batch_size=16, verbose=2, show_accuracy=True)
'''
Train on 37800 samples, validate on 4200 samples
Epoch 0
loss: 0.0190 - acc.: 0.8750
Epoch 1
loss: 0.0146 - acc.: 0.8750
Epoch 2
loss: 0.0049 - acc.: 1.0000
'''

'''
Demonstration of the validation_split argument
'''
model.fit(X_train, y_train, nb_epoch=3, batch_size=16,
          validation_split=0.1, show_accuracy=True, verbose=1)
'''
Train on 37800 samples, validate on 4200 samples
Epoch 0
37800/37800 [==============================] - 7s - loss: 0.0385 - acc.: 0.7258 - val. loss: 0.0160 - val. acc.: 0.9136
Epoch 1
37800/37800 [==============================] - 8s - loss: 0.0140 - acc.: 0.9265 - val. loss: 0.0109 - val. acc.: 0.9383
Epoch 2
10960/37800 [=======&gt;......................] - ETA: 4s - loss: 0.0109 - acc.: 0.9420
'''
</code></pre>

<h1 id="using-the-graph-model">Using the Graph model</h1>
<pre><code class="python"># graph model with one input and two outputs
graph = Graph()
graph.add_input(name='input', input_shape=(32,))
graph.add_node(Dense(16), name='dense1', input='input')
graph.add_node(Dense(4), name='dense2', input='input')
graph.add_node(Dense(4), name='dense3', input='dense1')
graph.add_output(name='output1', input='dense2')
graph.add_output(name='output2', input='dense3')

graph.compile(optimizer='rmsprop', loss={'output1':'mse', 'output2':'mse'})
history = graph.fit({'input':X_train, 'output1':y_train, 'output2':y2_train}, nb_epoch=10)

</code></pre>

<pre><code class="python"># graph model with two inputs and one output
graph = Graph()
graph.add_input(name='input1', input_shape=(32,))
graph.add_input(name='input2', input_shape=(32,))
graph.add_node(Dense(16), name='dense1', input='input1')
graph.add_node(Dense(4), name='dense2', input='input2')
graph.add_node(Dense(4), name='dense3', input='dense1')
graph.add_output(name='output', inputs=['dense2', 'dense3'], merge_mode='sum')
graph.compile(optimizer='rmsprop', loss={'output':'mse'})

history = graph.fit({'input1':X_train, 'input2':X2_train, 'output':y_train}, nb_epoch=10)
predictions = graph.predict({'input1':X_test, 'input2':X2_test}) # {'output':...}

</code></pre>

<hr />
<h1 id="model-api-documentation">Model API documentation</h1>
<p><span style="float:right;"><a href="https://github.com/fchollet/keras/blob/master/keras/models.py#L470">[source]</a></span></p>
<h1 id="sequential">Sequential</h1>
<pre><code class="python">keras.layers.containers.Sequential(layers=[])
</code></pre>

<p>Linear stack of layers.</p>
<p>Inherits from containers.Sequential.</p>
<h3 id="methods">Methods</h3>
<pre><code class="python">compile(optimizer, loss, class_mode=None, sample_weight_mode=None)
</code></pre>

<p>Configure the learning process.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>optimizer</strong>: str (name of optimizer) or optimizer object.
    See <a href="../optimizers/">optimizers</a>.</li>
<li><strong>loss</strong>: str (name of objective function) or objective function.
    See <a href="../objectives/">objectives</a>.</li>
<li><strong>class_mode</strong>: deprecated argument,
    it is set automatically starting with Keras 0.3.3.</li>
<li><strong>sample_weight_mode</strong>: if you need to do timestep-wise
    sample weighting (2D weights), set this to "temporal".
    "None" defaults to sample-wise weights (1D).</li>
<li><strong>kwargs</strong>: for Theano backend, these are passed into K.function.
    Ignored for Tensorflow backend.</li>
</ul>
<pre><code class="python">evaluate(X, y, batch_size=128, show_accuracy=False, verbose=1, sample_weight=None)
</code></pre>

<p>Compute the loss on some input data, batch by batch.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>X</strong>: input data, as a numpy array.</li>
<li><strong>y</strong>: labels, as a numpy array.</li>
<li><strong>batch_size</strong>: integer.</li>
<li><strong>show_accuracy</strong>: boolean.</li>
<li><strong>verbose</strong>: verbosity mode, 0 or 1.</li>
<li><strong>sample_weight</strong>: sample weights, as a numpy array.</li>
</ul>
<pre><code class="python">evaluate_generator(generator, val_samples, show_accuracy=False, verbose=1)
</code></pre>

<p>Evaluates the model on a generator. The generator should
return the same kind of data with every yield as accepted
by <code>evaluate</code></p>
<ul>
<li><strong>Arguments</strong>:</li>
<li><strong>generator</strong>:
    generator yielding dictionaries of the kind accepted
    by <code>evaluate</code>, or tuples of such dictionaries and
    associated dictionaries of sample weights.</li>
<li><strong>val_samples</strong>:
    total number of samples to generate from <code>generator</code>
    to use in validation.</li>
<li><strong>show_accuracy</strong>: whether to display accuracy in logs.</li>
<li><strong>verbose</strong>: verbosity mode, 0 (silent), 1 (per-batch logs),
    or 2 (per-epoch logs).</li>
</ul>
<pre><code class="python">fit(X, y, batch_size=128, nb_epoch=100, verbose=1, callbacks=[], validation_split=0.0, validation_data=None, shuffle=True, show_accuracy=False, class_weight=None, sample_weight=None)
</code></pre>

<p>Train the model for a fixed number of epochs.</p>
<p>Returns a history object. Its <code>history</code> attribute is a record of
training loss values at successive epochs,
as well as validation loss values (if applicable).</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>X</strong>: data, as a numpy array.</li>
<li><strong>y</strong>: labels, as a numpy array.</li>
<li><strong>batch_size</strong>: int. Number of samples per gradient update.</li>
<li><strong>nb_epoch</strong>: int.</li>
<li><strong>verbose</strong>: 0 for no logging to stdout,
    1 for progress bar logging, 2 for one log line per epoch.</li>
<li><strong>callbacks</strong>: <code>keras.callbacks.Callback</code> list.
    List of callbacks to apply during training.
    See <a href="../callbacks/">callbacks</a>.</li>
<li><strong>validation_split</strong>: float (0. &lt; x &lt; 1).
    Fraction of the data to use as held-out validation data.</li>
<li><strong>validation_data</strong>: tuple (X, y) to be used as held-out
    validation data. Will override validation_split.</li>
<li><strong>shuffle</strong>: boolean or str (for 'batch').
    Whether to shuffle the samples at each epoch.
    'batch' is a special option for dealing with the
    limitations of HDF5 data; it shuffles in batch-sized chunks.</li>
<li><strong>show_accuracy</strong>: boolean. Whether to display
    class accuracy in the logs to stdout at each epoch.</li>
<li><strong>class_weight</strong>: dictionary mapping classes to a weight value,
    used for scaling the loss function (during training only).</li>
<li><strong>sample_weight</strong>: list or numpy array of weights for
    the training samples, used for scaling the loss function
    (during training only). You can either pass a flat (1D)
    Numpy array with the same length as the input samples<ul>
<li><strong>(1</strong>:1 mapping between weights and samples),
or in the case of temporal data,
you can pass a 2D array with shape (samples, sequence_length),
to apply a different weight to every timestep of every sample.
In this case you should make sure to specify
sample_weight_mode="temporal" in compile().</li>
</ul>
</li>
</ul>
<pre><code class="python">fit_generator(generator, samples_per_epoch, nb_epoch, verbose=1, show_accuracy=False, callbacks=[], validation_data=None, nb_val_samples=None, class_weight=None, nb_worker=1, nb_val_worker=None)
</code></pre>

<p>Fit a model on data generated batch-by-batch by a Python generator.
The generator is run in parallel to the model, for efficiency,
and can be run by multiple workers at the same time.
For instance, this allows you to do real-time data augmentation
on images on CPU in parallel to training your model on GPU.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>generator</strong>: a Python generator,
    yielding either (X, y) or (X, y, sample_weight).
    The generator is expected to loop over its data
    indefinitely. An epoch finishes when <code>samples_per_epoch</code>
    samples have been seen by the model.
    The output of the generator must be a tuple of either 2 or 3
    numpy arrays.
    If the output tuple has two elements, they are assumed to be
    (input_data, target_data).
    If it has three elements, they are assumed to be
    (input_data, target_data, sample_weight).
    All arrays should contain the same number of samples.</li>
<li><strong>samples_per_epoch</strong>: integer, number of samples to process before
    starting a new epoch.</li>
<li><strong>nb_epoch</strong>: integer, total number of iterations on the data.</li>
<li><strong>verbose</strong>: verbosity mode, 0, 1, or 2.</li>
<li><strong>show_accuracy</strong>: boolean. Whether to display accuracy (only relevant
    for classification problems).</li>
<li><strong>callbacks</strong>: list of callbacks to be called during training.</li>
<li><strong>validation_data</strong>: tuple of 2 or 3 numpy arrays, or a generator.
    If 2 elements, they are assumed to be (input_data, target_data);
    if 3 elements, they are assumed to be
    (input_data, target_data, sample weights). If generator,
    it is assumed to yield tuples of 2 or 3 elements as above.
    The generator will be called at the end of every epoch until
    at least <code>nb_val_samples</code> examples have been obtained,
    with these examples used for validation.</li>
<li><strong>nb_val_samples</strong>: number of samples to use from validation
    generator at the end of every epoch.</li>
<li><strong>class_weight</strong>: dictionary mapping class indices to a weight
    for the class.</li>
<li><strong>nb_worker</strong>: integer, number of workers to use for running
    the generator (in parallel to model training).
    If using multiple workers, the processing order of batches
    generated by the model will be non-deterministic.
    If using multiple workers, make sure to protect
    any thread-unsafe operation done by the generator
    using a Python mutex.</li>
<li><strong>nb_val_worker</strong>: same as <code>nb_worker</code>, except for validation data.
    Has no effect if no validation data or validation data is
    not a generator. If <code>nb_val_worker</code> is None, defaults to
    <code>nb_worker</code>.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A <code>History</code> object.</p>
<p><strong>Examples</strong></p>
<pre><code class="python">def generate_arrays_from_file(path):
    while 1:
    f = open(path)
    for line in f:
        # create numpy arrays of input data
        # and labels, from each line in the file
        x, y = process_line(line)
        yield x, y
    f.close()

model.fit_generator(generate_arrays_from_file('/my_file.txt'),
        samples_per_epoch=10000, nb_epoch=10)
</code></pre>

<pre><code class="python">load_weights(filepath)
</code></pre>

<p>Load all layer weights from a HDF5 save file.</p>
<pre><code class="python">predict(X, batch_size=128, verbose=0)
</code></pre>

<p>Generate output predictions for the input samples
batch by batch.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>X</strong>: the input data, as a numpy array.</li>
<li><strong>batch_size</strong>: integer.</li>
<li><strong>verbose</strong>: verbosity mode, 0 or 1.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A numpy array of predictions.</p>
<pre><code class="python">predict_classes(X, batch_size=128, verbose=1)
</code></pre>

<p>Generate class predictions for the input samples
batch by batch.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>X</strong>: the input data, as a numpy array.</li>
<li><strong>batch_size</strong>: integer.</li>
<li><strong>verbose</strong>: verbosity mode, 0 or 1.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A numpy array of class predictions.</p>
<pre><code class="python">predict_on_batch(X)
</code></pre>

<p>Returns predictions for a single batch of samples.</p>
<pre><code class="python">predict_proba(X, batch_size=128, verbose=1)
</code></pre>

<p>Generate class probability predictions for the input samples
batch by batch.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>X</strong>: the input data, as a numpy array.</li>
<li><strong>batch_size</strong>: integer.</li>
<li><strong>verbose</strong>: verbosity mode, 0 or 1.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A numpy array of probability predictions.</p>
<pre><code class="python">save_weights(filepath, overwrite=False)
</code></pre>

<p>Dump all layer weights to a HDF5 file.</p>
<pre><code class="python">test_on_batch(X, y, accuracy=False, sample_weight=None)
</code></pre>

<p>Returns the loss over a single batch of samples,
or a tuple <code>(loss, accuracy)</code> if <code>accuracy=True</code>.</p>
<ul>
<li><strong>Arguments</strong>: see <code>fit</code> method.</li>
</ul>
<pre><code class="python">train_on_batch(X, y, accuracy=False, class_weight=None, sample_weight=None)
</code></pre>

<p>Single gradient update over one batch of samples.</p>
<p>Returns the loss over the data,
or a tuple <code>(loss, accuracy)</code> if <code>accuracy=True</code>.</p>
<ul>
<li><strong>Arguments</strong>: see <code>fit</code> method.</li>
</ul>
<pre><code class="python">add(layer)
</code></pre>

<p>Defined by <a href="http://keras.io/layers/containers#sequential">Sequential</a>.</p>
<pre><code class="python">clear_previous(reset_weights=True)
</code></pre>

<p>Defined by <a href="http://keras.io/layers/core#layer">Layer</a>.</p>
<pre><code class="python">count_params()
</code></pre>

<p>Defined by <a href="http://keras.io/layers/core#layer">Layer</a>.</p>
<pre><code class="python">get_config(verbose=0)
</code></pre>

<p>Defined by <a href="http://keras.io/layers/core#layer">Layer</a>.</p>
<pre><code class="python">get_input(train=False)
</code></pre>

<p>Defined by <a href="http://keras.io/layers/core#layer">Layer</a>.</p>
<pre><code class="python">get_output(train=False)
</code></pre>

<p>Defined by <a href="http://keras.io/layers/core#layer">Layer</a>.</p>
<pre><code class="python">get_output_mask(train=None)
</code></pre>

<p>Defined by <a href="http://keras.io/layers/core#layer">Layer</a>.</p>
<pre><code class="python">get_weights()
</code></pre>

<p>Defined by <a href="http://keras.io/layers/core#layer">Layer</a>.</p>
<pre><code class="python">reset_states()
</code></pre>

<p>Defined by <a href="http://keras.io/layers/containers#sequential">Sequential</a>.</p>
<pre><code class="python">set_input()
</code></pre>

<p>Defined by <a href="http://keras.io/layers/containers#sequential">Sequential</a>.</p>
<pre><code class="python">set_input_shape(input_shape)
</code></pre>

<p>Defined by <a href="http://keras.io/layers/core#layer">Layer</a>.</p>
<pre><code class="python">set_previous(layer, reset_weights=True)
</code></pre>

<p>Defined by <a href="http://keras.io/layers/core#layer">Layer</a>.</p>
<pre><code class="python">set_weights(weights)
</code></pre>

<p>Defined by <a href="http://keras.io/layers/core#layer">Layer</a>.</p>
<pre><code class="python">summary()
</code></pre>

<p>Defined by <a href="http://keras.io/models#model">Model</a>.</p>
<pre><code class="python">supports_masked_input()
</code></pre>

<p>Defined by <a href="http://keras.io/layers/core#layer">Layer</a>.</p>
<pre><code class="python">to_json()
</code></pre>

<p>Defined by <a href="http://keras.io/models#model">Model</a>.</p>
<pre><code class="python">to_yaml()
</code></pre>

<p>Defined by <a href="http://keras.io/models#model">Model</a>.</p>
<hr />
<p><span style="float:right;"><a href="https://github.com/fchollet/keras/blob/master/keras/models.py#L1192">[source]</a></span></p>
<h1 id="graph">Graph</h1>
<pre><code class="python">keras.layers.containers.Graph()
</code></pre>

<p>Arbitrary connection graph.
It can have any number of inputs and outputs,
with each output trained with its own loss function.
The quantity being optimized by a Graph model is
the sum of all loss functions over the different outputs.</p>
<p>Inherits from <code>containers.Graph</code>.</p>
<h3 id="methods_1">Methods</h3>
<pre><code class="python">compile(optimizer, loss, sample_weight_modes={}, loss_weights={})
</code></pre>

<p>Configure the learning process.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>optimizer</strong>: str (name of optimizer) or optimizer object.
    See <a href="../optimizers/">optimizers</a>.</li>
<li><strong>loss</strong>: dictionary mapping the name(s) of the output(s) to
    a loss function (string name of objective function or
    objective function. See <a href="../objectives/">objectives</a>).</li>
<li><strong>sample_weight_modes</strong>: optional dictionary mapping certain
    output names to a sample weight mode ("temporal" and None
    are the only supported modes). If you need to do
    timestep-wise loss weighting on one of your graph outputs,
    you will need to set the sample weight mode for this output
    to "temporal".</li>
<li><strong>loss_weights</strong>: dictionary you can pass to specify a weight
    coefficient for each loss function (in a multi-output model).
    If no loss weight is specified for an output,
    the weight for this output's loss will be considered to be 1.</li>
<li><strong>kwargs</strong>: for Theano backend, these are passed into K.function.
    Ignored for Tensorflow backend.</li>
</ul>
<pre><code class="python">evaluate(data, batch_size=128, show_accuracy=False, verbose=0, sample_weight={})
</code></pre>

<p>Compute the loss on some input data, batch by batch.</p>
<p>Returns the loss over the data,
or a tuple <code>(loss, accuracy)</code> if <code>show_accuracy=True</code>.</p>
<ul>
<li><strong>Arguments</strong>: see <code>fit</code> method.</li>
</ul>
<pre><code class="python">evaluate_generator(generator, nb_val_samples, show_accuracy=False, verbose=1)
</code></pre>

<p>Evaluates the model on a generator. The generator should
return the same kind of data with every yield as accepted
by <code>evaluate</code>.</p>
<p>If <code>show_accuracy</code>, it returns a tuple <code>(loss, accuracy)</code>,
otherwise it returns the loss value.</p>
<ul>
<li><strong>Arguments</strong>:</li>
<li><strong>generator</strong>:
    generator yielding dictionaries of the kind accepted
    by <code>evaluate</code>, or tuples of such dictionaries and
    associated dictionaries of sample weights.</li>
<li><strong>nb_val_samples</strong>:
    total number of samples to generate from <code>generator</code>
    to use in validation.</li>
<li><strong>show_accuracy</strong>: whether to log accuracy.
    Can only be used if your Graph has a single output (otherwise "accuracy"
    is ill-defined).</li>
</ul>
<p>Other arguments are the same as for <code>fit</code>.</p>
<pre><code class="python">fit(data, batch_size=128, nb_epoch=100, verbose=1, callbacks=[], validation_split=0.0, validation_data=None, shuffle=True, show_accuracy=False, class_weight={}, sample_weight={})
</code></pre>

<p>Train the model for a fixed number of epochs.</p>
<p>Returns a history object. Its <code>history</code> attribute is a record of
training loss values at successive epochs,
as well as validation loss values (if applicable).</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>data</strong>: dictionary mapping input names and outputs names to
    appropriate numpy arrays. All arrays should contain
    the same number of samples.</li>
<li><strong>batch_size</strong>: int. Number of samples per gradient update.</li>
<li><strong>nb_epoch</strong>: int.</li>
<li><strong>verbose</strong>: 0 for no logging to stdout,
    1 for progress bar logging, 2 for one log line per epoch.</li>
<li><strong>callbacks</strong>: <code>keras.callbacks.Callback</code> list. List of callbacks
    to apply during training. See <a href="../callbacks/">callbacks</a>.</li>
<li><strong>validation_split</strong>: float (0. &lt; x &lt; 1). Fraction of the data to
    use as held-out validation data.</li>
<li><strong>validation_data</strong>: dictionary mapping input names and outputs names
    to appropriate numpy arrays to be used as
    held-out validation data.
    All arrays should contain the same number of samples.
    Will override validation_split.</li>
<li><strong>shuffle</strong>: boolean. Whether to shuffle the samples at each epoch.</li>
<li><strong>show_accuracy</strong>: whether to log accuracy.
    Can only be used if your Graph has a single output (otherwise "accuracy"
    is ill-defined).</li>
<li><strong>class_weight</strong>: dictionary mapping output names to
    class weight dictionaries.</li>
<li><strong>sample_weight</strong>: dictionary mapping output names to
    numpy arrays of sample weights.</li>
</ul>
<pre><code class="python">fit_generator(generator, samples_per_epoch, nb_epoch, verbose=1, show_accuracy=False, callbacks=[], validation_data=None, nb_val_samples=None, class_weight={}, nb_worker=1, nb_val_worker=None)
</code></pre>

<p>Fit a model on data generated batch-by-batch by a Python generator.
The generator is run in parallel to the model, for efficiency,
and can be run by multiple workers at the same time.
For instance, this allows you to do real-time data augmentation
on images on CPU in parallel to training your model on GPU.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>generator</strong>: a generator.
    The output of the generator must be either a dictionary
    mapping inputs and outputs names to numpy arrays, or
    a tuple of dictionaries (input_data, sample_weight).
    All arrays should contain the same number of samples.
    The generator is expected to loop over its data
    indefinitely. An epoch finishes when <code>samples_per_epoch</code>
    samples have been seen by the model.</li>
<li><strong>samples_per_epoch</strong>: integer, number of samples to process before
    going to the next epoch.</li>
<li><strong>nb_epoch</strong>: integer, total number of iterations on the data.</li>
<li><strong>verbose</strong>: verbosity mode, 0, 1, or 2.</li>
<li><strong>show_accuracy</strong>: whether to log accuracy.
    Can only be used if your Graph has a single output (otherwise "accuracy"
    is ill-defined).</li>
<li><strong>callbacks</strong>: list of callbacks to be called during training.</li>
<li><strong>validation_data</strong>: dictionary mapping input names and outputs names
    to appropriate numpy arrays to be used as
    held-out validation data, or a generator yielding such
    dictionaries. All arrays should contain the same number
    of samples. If a generator, will be called until more than
    <code>nb_val_samples</code> examples have been generated at the
    end of every epoch. These examples will then be used
    as the validation data.</li>
<li><strong>nb_val_samples</strong>: number of samples to use from validation
    generator at the end of every epoch.</li>
<li><strong>class_weight</strong>: dictionary mapping class indices to a weight
    for the class.</li>
<li><strong>nb_worker</strong>: integer, number of workers to use for running
    the generator (in parallel to model training).
    If using multiple workers, the processing order of batches
    generated by the model will be non-deterministic.
    If using multiple workers, make sure to protect
    any thread-unsafe operation done by the generator
    using a Python mutex.</li>
<li><strong>nb_val_worker</strong>: same as <code>nb_worker</code>, except for validation data.
    Has no effect if no validation data or validation data is
    not a generator. If <code>None</code>, defaults to nb_worker.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A <code>History</code> object.</p>
<p><strong>Examples</strong></p>
<pre><code class="python">def generate_arrays_from_file(path):
    while 1:
    f = open(path)
    for line in f:
        # create numpy arrays of input data
        # and labels, from each line in the file
        x1, x2, y = process_line(line)
        yield {'input_1': x1, 'input_2': x2, 'output': y}
    f.close()

graph.fit_generator(generate_arrays_from_file('/my_file.txt'),
        samples_per_epoch=10000, nb_epoch=10)
</code></pre>

<pre><code class="python">load_weights(filepath)
</code></pre>

<p>Load weights from a HDF5 file.</p>
<pre><code class="python">predict(data, batch_size=128, verbose=0)
</code></pre>

<p>Generate output predictions for the input samples
batch by batch.</p>
<ul>
<li><strong>Arguments</strong>: see <code>fit</code> method.</li>
</ul>
<pre><code class="python">predict_on_batch(data)
</code></pre>

<p>Generate predictions for a single batch of samples.</p>
<pre><code class="python">save_weights(filepath, overwrite=False)
</code></pre>

<p>Save weights from all layers to a HDF5 files.</p>
<pre><code class="python">test_on_batch(data, accuracy=False, sample_weight={})
</code></pre>

<p>Test the network on a single batch of samples.</p>
<p>If <code>accuracy</code>, it returns a tuple <code>(loss, accuracy)</code>,
otherwise it returns the loss value.</p>
<ul>
<li><strong>Arguments</strong>: see <code>fit</code> method.</li>
</ul>
<pre><code class="python">train_on_batch(data, accuracy=False, class_weight={}, sample_weight={})
</code></pre>

<p>Single gradient update on a batch of samples.</p>
<p>Returns the loss over the data,
or a tuple <code>(loss, accuracy)</code> if <code>accuracy=True</code>.</p>
<ul>
<li><strong>Arguments</strong>: see <code>fit</code> method.</li>
</ul>
<pre><code class="python">add_input(name, input_shape=None, batch_input_shape=None, dtype='float')
</code></pre>

<p>Defined by <a href="http://keras.io/layers/containers#graph">Graph</a>.</p>
<pre><code class="python">add_node(layer, name, input=None, inputs=[], merge_mode='concat', concat_axis=-1, dot_axes=-1, create_output=False)
</code></pre>

<p>Defined by <a href="http://keras.io/layers/containers#graph">Graph</a>.</p>
<pre><code class="python">add_output(name, input=None, inputs=[], merge_mode='concat', concat_axis=-1, dot_axes=-1)
</code></pre>

<p>Defined by <a href="http://keras.io/layers/containers#graph">Graph</a>.</p>
<pre><code class="python">add_shared_node(layer, name, inputs=[], merge_mode=None, concat_axis=-1, dot_axes=-1, outputs=[], create_output=False)
</code></pre>

<p>Defined by <a href="http://keras.io/layers/containers#graph">Graph</a>.</p>
<pre><code class="python">clear_previous(reset_weights=True)
</code></pre>

<p>Defined by <a href="http://keras.io/layers/core#layer">Layer</a>.</p>
<pre><code class="python">count_params()
</code></pre>

<p>Defined by <a href="http://keras.io/layers/core#layer">Layer</a>.</p>
<pre><code class="python">get_config(verbose=0)
</code></pre>

<p>Defined by <a href="http://keras.io/layers/core#layer">Layer</a>.</p>
<pre><code class="python">get_input(train=False)
</code></pre>

<p>Defined by <a href="http://keras.io/layers/core#layer">Layer</a>.</p>
<pre><code class="python">get_output(train=False)
</code></pre>

<p>Defined by <a href="http://keras.io/layers/core#layer">Layer</a>.</p>
<pre><code class="python">get_output_mask(train=None)
</code></pre>

<p>Defined by <a href="http://keras.io/layers/core#layer">Layer</a>.</p>
<pre><code class="python">get_weights()
</code></pre>

<p>Defined by <a href="http://keras.io/layers/core#layer">Layer</a>.</p>
<pre><code class="python">reset_states()
</code></pre>

<p>Defined by <a href="http://keras.io/layers/containers#graph">Graph</a>.</p>
<pre><code class="python">set_input_shape(input_shape)
</code></pre>

<p>Defined by <a href="http://keras.io/layers/core#layer">Layer</a>.</p>
<pre><code class="python">set_previous(layer, connection_map={}, reset_weights=True)
</code></pre>

<p>Defined by <a href="http://keras.io/layers/core#layer">Layer</a>.</p>
<pre><code class="python">set_weights(weights)
</code></pre>

<p>Defined by <a href="http://keras.io/layers/core#layer">Layer</a>.</p>
<pre><code class="python">summary()
</code></pre>

<p>Defined by <a href="http://keras.io/models#model">Model</a>.</p>
<pre><code class="python">supports_masked_input()
</code></pre>

<p>Defined by <a href="http://keras.io/layers/core#layer">Layer</a>.</p>
<pre><code class="python">to_json()
</code></pre>

<p>Defined by <a href="http://keras.io/models#model">Model</a>.</p>
<pre><code class="python">to_yaml()
</code></pre>

<p>Defined by <a href="http://keras.io/models#model">Model</a>.</p>
<hr />
<p><span style="float:right;"><a href="https://github.com/fchollet/keras/blob/master/keras/models.py#L257">[source]</a></span></p>
<h1 id="model">Model</h1>
<pre><code class="python">keras.models.Model()
</code></pre>

<p>Abstract base model class.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../activations/" class="btn btn-neutral float-right" title="Activations">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../objectives/" class="btn btn-neutral" title="Objectives"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="http://github.com/fchollet/keras" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../objectives/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../activations/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../js/theme.js"></script>

</body>
</html>
