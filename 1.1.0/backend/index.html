<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Backend - Keras Documentation</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Backend";
    var mkdocs_page_input_path = "backend.md";
    var mkdocs_page_url = "/backend/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-61785484-1', 'keras.io');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Keras Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Getting started</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../getting-started/sequential-model-guide/">Guide to the Sequential model</a>
                </li>
                <li class="">
                    
    <a class="" href="../getting-started/functional-api-guide/">Guide to the Functional API</a>
                </li>
                <li class="">
                    
    <a class="" href="../getting-started/faq/">FAQ</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Models</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../models/about-keras-models/">About Keras models</a>
                </li>
                <li class="">
                    
    <a class="" href="../models/sequential/">Sequential</a>
                </li>
                <li class="">
                    
    <a class="" href="../models/model/">Model (functional API)</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Layers</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../layers/about-keras-layers/">About Keras layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/core/">Core Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/convolutional/">Convolutional Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/pooling/">Pooling Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/local/">Locally-connected Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/recurrent/">Recurrent Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/embeddings/">Embedding Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/advanced-activations/">Advanced Activations Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/normalization/">Normalization Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/noise/">Noise layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/wrappers/">Layer wrappers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/writing-your-own-keras-layers/">Writing your own Keras layers</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Preprocessing</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../preprocessing/sequence/">Sequence Preprocessing</a>
                </li>
                <li class="">
                    
    <a class="" href="../preprocessing/text/">Text Preprocessing</a>
                </li>
                <li class="">
                    
    <a class="" href="../preprocessing/image/">Image Preprocessing</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../objectives/">Objectives</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../optimizers/">Optimizers</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../activations/">Activations</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../callbacks/">Callbacks</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../datasets/">Datasets</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../applications/">Applications</a>
	    </li>
          
            <li class="toctree-l1 current">
		
    <a class="current" href="./">Backend</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#keras-backends">Keras backends</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#what-is-a-backend">What is a "backend"?</a></li>
        
            <li><a class="toctree-l3" href="#switching-from-one-backend-to-another">Switching from one backend to another</a></li>
        
            <li><a class="toctree-l3" href="#using-the-abstract-keras-backend-to-write-new-code">Using the abstract Keras backend to write new code</a></li>
        
            <li><a class="toctree-l3" href="#backend-functions">Backend functions</a></li>
        
        </ul>
    

    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../initializations/">Initializations</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../regularizers/">Regularizers</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../constraints/">Constraints</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../visualization/">Visualization</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../scikit-learn-api/">Scikit-learn API</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Keras Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Backend</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="http://github.com/fchollet/keras/edit/master/docs/backend.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="keras-backends">Keras backends</h1>
<h2 id="what-is-a-backend">What is a "backend"?</h2>
<p>Keras is a model-level library, providing high-level building blocks for developing deep learning models. It does not handle itself low-level operations such as tensor products, convolutions and so on. Instead, it relies on a specialized, well-optimized tensor manipulation library to do so, serving as the "backend engine" of Keras. Rather than picking one single tensor library and making the implementation of Keras tied to that library, Keras handles the problem in a modular way, and several different backend engines can be plugged seamlessly into Keras.</p>
<p>At this time, Keras has two backend implementations available: the <strong>TensorFlow</strong> backend and the <strong>Theano</strong> backend.</p>
<ul>
<li><a href="http://www.tensorflow.org/">TensorFlow</a> is an open-source symbolic tensor manipulation framework developed by Google, Inc.</li>
<li><a href="http://deeplearning.net/software/theano/">Theano</a> is an open-source symbolic tensor manipulation framework developed by LISA/MILA Lab at Université de Montréal.</li>
</ul>
<p>In the future, we are likely to add more backend options. If you are interested in developing a new backend, get in touch!</p>
<hr />
<h2 id="switching-from-one-backend-to-another">Switching from one backend to another</h2>
<p>If you have run Keras at least once, you will find the Keras configuration file at:</p>
<p><code>~/.keras/keras.json</code></p>
<p>If it isn't there, you can create it.</p>
<p>The default configuration file looks like this:</p>
<pre><code>{
    &quot;image_dim_ordering&quot;: &quot;tf&quot;,
    &quot;epsilon&quot;: 1e-07,
    &quot;floatx&quot;: &quot;float32&quot;,
    &quot;backend&quot;: &quot;tensorflow&quot;
}
</code></pre>

<p>Simply change the field <code>backend</code> to either <code>"theano"</code> or <code>"tensorflow"</code>, and Keras will use the new configuration next time you run any Keras code.</p>
<p>You can also define the environment variable <code>KERAS_BACKEND</code> and this will
override what is defined in your config file :</p>
<pre><code class="bash">KERAS_BACKEND=tensorflow python -c &quot;from keras import backend&quot;
Using TensorFlow backend.
</code></pre>

<hr />
<h2 id="using-the-abstract-keras-backend-to-write-new-code">Using the abstract Keras backend to write new code</h2>
<p>If you want the Keras modules you write to be compatible with both Theano and TensorFlow, you have to write them via the abstract Keras backend API. Here's an intro.</p>
<p>You can import the backend module via:</p>
<pre><code class="python">from keras import backend as K
</code></pre>

<p>The code below instantiates an input placeholder. It's equivalent to <code>tf.placeholder()</code> or <code>T.matrix()</code>, <code>T.tensor3()</code>, etc.</p>
<pre><code class="python">input = K.placeholder(shape=(2, 4, 5))
# also works:
input = K.placeholder(shape=(None, 4, 5))
# also works:
input = K.placeholder(ndim=3)
</code></pre>

<p>The code below instantiates a shared variable. It's equivalent to <code>tf.variable()</code> or <code>theano.shared()</code>.</p>
<pre><code class="python">val = np.random.random((3, 4, 5))
var = K.variable(value=val)

# all-zeros variable:
var = K.zeros(shape=(3, 4, 5))
# all-ones:
var = K.ones(shape=(3, 4, 5))
</code></pre>

<p>Most tensor operations you will need can be done as you would in TensorFlow or Theano:</p>
<pre><code class="python">a = b + c * K.abs(d)
c = K.dot(a, K.transpose(b))
a = K.sum(b, axis=2)
a = K.softmax(b)
a = concatenate([b, c], axis=-1)
# etc...
</code></pre>

<hr />
<h2 id="backend-functions">Backend functions</h2>
<h3 id="epsilon">epsilon</h3>
<pre><code class="python">epsilon()
</code></pre>

<p>Returns the value of the fuzz
factor used in numeric expressions.</p>
<hr />
<h3 id="set_epsilon">set_epsilon</h3>
<pre><code class="python">set_epsilon(e)
</code></pre>

<p>Sets the value of the fuzz
factor used in numeric expressions.</p>
<hr />
<h3 id="floatx">floatx</h3>
<pre><code class="python">floatx()
</code></pre>

<p>Returns the default float type, as a string
(e.g. 'float16', 'float32', 'float64').</p>
<hr />
<h3 id="cast_to_floatx">cast_to_floatx</h3>
<pre><code class="python">cast_to_floatx(x)
</code></pre>

<p>Cast a Numpy array to floatx.</p>
<hr />
<h3 id="image_dim_ordering">image_dim_ordering</h3>
<pre><code class="python">image_dim_ordering()
</code></pre>

<p>Returns the image dimension ordering
convention ('th' or 'tf').</p>
<hr />
<h3 id="set_image_dim_ordering">set_image_dim_ordering</h3>
<pre><code class="python">set_image_dim_ordering(dim_ordering)
</code></pre>

<p>Sets the value of the image dimension
ordering convention ('th' or 'tf').</p>
<hr />
<h3 id="variable">variable</h3>
<pre><code class="python">variable(value, dtype='float32', name=None)
</code></pre>

<p>Instantiate a tensor variable.</p>
<hr />
<h3 id="placeholder">placeholder</h3>
<pre><code class="python">placeholder(shape=None, ndim=None, dtype='float32', sparse=False, name=None)
</code></pre>

<p>Instantiate an input data placeholder variable.</p>
<hr />
<h3 id="shape">shape</h3>
<pre><code class="python">shape(x)
</code></pre>

<p>Return the shape of a tensor.</p>
<ul>
<li><strong>Warning</strong>: type returned will be different for
Theano backend (Theano tensor type) and TF backend (TF TensorShape).</li>
</ul>
<hr />
<h3 id="eval">eval</h3>
<pre><code class="python">eval(x)
</code></pre>

<p>Run a graph.</p>
<hr />
<h3 id="zeros">zeros</h3>
<pre><code class="python">zeros(shape, dtype='float32', name=None)
</code></pre>

<p>Instantiate an all-zeros variable.</p>
<hr />
<h3 id="ones">ones</h3>
<pre><code class="python">ones(shape, dtype='float32', name=None)
</code></pre>

<p>Instantiate an all-ones variable.</p>
<hr />
<h3 id="eye">eye</h3>
<pre><code class="python">eye(size, dtype='float32', name=None)
</code></pre>

<p>Instantiate an identity matrix.</p>
<hr />
<h3 id="count_params">count_params</h3>
<pre><code class="python">count_params(x)
</code></pre>

<p>Return number of scalars in a tensor.</p>
<ul>
<li><strong>Return</strong>: numpy integer.</li>
</ul>
<hr />
<h3 id="batch_dot">batch_dot</h3>
<pre><code class="python">batch_dot(x, y, axes=None)
</code></pre>

<p>Batchwise dot product.</p>
<p>batch_dot results in a tensor with less dimensions than the input.
If the number of dimensions is reduced to 1, we use <code>expand_dims</code> to
make sure that ndim is at least 2.</p>
<p><strong>Arguments</strong></p>
<p>x, y: tensors with ndim &gt;= 2
- <strong>axes</strong>: list (or single) int with target dimensions</p>
<p><strong>Returns</strong></p>
<p>A tensor with shape equal to the concatenation of x's shape
(less the dimension that was summed over) and y's shape
(less the batch dimension and the dimension that was summed over).
If the final rank is 1, we reshape it to (batch_size, 1).</p>
<p><strong>Examples</strong></p>
<p>Assume x = [[1, 2], [3, 4]]   and y = [[5, 6], [7, 8]]
batch_dot(x, y, axes=1) = [[17, 53]] which is the main diagonal
of x.dot(y.T), although we never have to calculate the off-diagonal
elements.</p>
<p>Shape inference:
Let x's shape be (100, 20) and y's shape be (100, 30, 20).
If dot_axes is (1, 2), to find the output shape of resultant tensor,
loop through each dimension in x's shape and y's shape:
x.shape[0] : 100 : append to output shape
x.shape[1] : 20 : do not append to output shape,
dimension 1 of x has been summed over. (dot_axes[0] = 1)
y.shape[0] : 100 : do not append to output shape,
always ignore first dimension of y
y.shape[1] : 30 : append to output shape
y.shape[2] : 20 : do not append to output shape,
dimension 2 of y has been summed over. (dot_axes[1] = 2)</p>
<p>output_shape = (100, 30)</p>
<hr />
<h3 id="gather">gather</h3>
<pre><code class="python">gather(reference, indices)
</code></pre>

<p>reference: a tensor.
- <strong>indices</strong>: an int tensor of indices.</p>
<ul>
<li><strong>Return</strong>: a tensor of same type as reference.</li>
</ul>
<hr />
<h3 id="sum">sum</h3>
<pre><code class="python">sum(x, axis=None, keepdims=False)
</code></pre>

<p>Sum of the values in a tensor, alongside the specified axis.</p>
<hr />
<h3 id="prod">prod</h3>
<pre><code class="python">prod(x, axis=None, keepdims=False)
</code></pre>

<p>Multiply the values in a tensor, alongside the specified axis.</p>
<hr />
<h3 id="any">any</h3>
<pre><code class="python">any(x, axis=None, keepdims=False)
</code></pre>

<p>Bitwise reduction (logical OR).</p>
<hr />
<h3 id="all">all</h3>
<pre><code class="python">all(x, axis=None, keepdims=False)
</code></pre>

<p>Bitwise reduction (logical AND).</p>
<hr />
<h3 id="normalize_batch_in_training">normalize_batch_in_training</h3>
<pre><code class="python">normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.0001)
</code></pre>

<p>Compute mean and std for batch then apply batch_normalization on batch.</p>
<hr />
<h3 id="batch_normalization">batch_normalization</h3>
<pre><code class="python">batch_normalization(x, mean, var, beta, gamma, epsilon=0.0001)
</code></pre>

<p>Apply batch normalization on x given mean, var, beta and gamma.</p>
<hr />
<h3 id="permute_dimensions">permute_dimensions</h3>
<pre><code class="python">permute_dimensions(x, pattern)
</code></pre>

<p>Transpose dimensions.</p>
<p>pattern should be a tuple or list of
dimension indices, e.g. [0, 2, 1].</p>
<hr />
<h3 id="repeat_elements">repeat_elements</h3>
<pre><code class="python">repeat_elements(x, rep, axis)
</code></pre>

<p>Repeat the elements of a tensor along an axis, like np.repeat.</p>
<p>If x has shape (s1, s2, s3) and axis=1, the output
will have shape (s1, s2 * rep, s3).</p>
<hr />
<h3 id="resize_images">resize_images</h3>
<pre><code class="python">resize_images(X, height_factor, width_factor, dim_ordering)
</code></pre>

<p>Resize the images contained in a 4D tensor of shape
- [batch, channels, height, width] (for 'th' dim_ordering)
- [batch, height, width, channels] (for 'tf' dim_ordering)
by a factor of (height_factor, width_factor). Both factors should be
positive integers.</p>
<hr />
<h3 id="resize_volumes">resize_volumes</h3>
<pre><code class="python">resize_volumes(X, depth_factor, height_factor, width_factor, dim_ordering)
</code></pre>

<p>Resize the volume contained in a 5D tensor of shape
- [batch, channels, depth, height, width] (for 'th' dim_ordering)
- [batch, depth, height, width, channels] (for 'tf' dim_ordering)
by a factor of (depth_factor, height_factor, width_factor).
Both factors should be positive integers.</p>
<hr />
<h3 id="repeat">repeat</h3>
<pre><code class="python">repeat(x, n)
</code></pre>

<p>Repeat a 2D tensor.</p>
<p>If x has shape (samples, dim) and n=2,
the output will have shape (samples, 2, dim).</p>
<hr />
<h3 id="batch_flatten">batch_flatten</h3>
<pre><code class="python">batch_flatten(x)
</code></pre>

<p>Turn a n-D tensor into a 2D tensor where
the first dimension is conserved.</p>
<hr />
<h3 id="expand_dims">expand_dims</h3>
<pre><code class="python">expand_dims(x, dim=-1)
</code></pre>

<p>Add a 1-sized dimension at index "dim".</p>
<hr />
<h3 id="squeeze">squeeze</h3>
<pre><code class="python">squeeze(x, axis)
</code></pre>

<p>Remove a 1-dimension from the tensor at index "axis".</p>
<hr />
<h3 id="temporal_padding">temporal_padding</h3>
<pre><code class="python">temporal_padding(x, padding=1)
</code></pre>

<p>Pad the middle dimension of a 3D tensor
with "padding" zeros left and right.</p>
<p>Apologies for the inane API, but Theano makes this
really hard.</p>
<hr />
<h3 id="spatial_2d_padding">spatial_2d_padding</h3>
<pre><code class="python">spatial_2d_padding(x, padding=(1, 1), dim_ordering='th')
</code></pre>

<p>Pad the 2nd and 3rd dimensions of a 4D tensor
with "padding[0]" and "padding[1]" (resp.) zeros left and right.</p>
<hr />
<h3 id="spatial_3d_padding">spatial_3d_padding</h3>
<pre><code class="python">spatial_3d_padding(x, padding=(1, 1, 1), dim_ordering='th')
</code></pre>

<p>Pad the 2nd, 3rd and 4th dimensions of a 5D tensor
with "padding[0]", "padding[1]" and "padding[2]" (resp.) zeros left and right.</p>
<hr />
<h3 id="one_hot">one_hot</h3>
<pre><code class="python">one_hot(indices, nb_classes)
</code></pre>

<p>Input: nD integer tensor of shape (batch_size, dim1, dim2, ... dim(n-1))
- <strong>Output</strong>: (n + 1)D one hot representation of the input
with shape (batch_size, dim1, dim2, ... dim(n-1), nb_classes)</p>
<hr />
<h3 id="reverse">reverse</h3>
<pre><code class="python">reverse(x, axes)
</code></pre>

<p>Reverse a tensor along the the specified axes</p>
<hr />
<h3 id="batch_get_value">batch_get_value</h3>
<pre><code class="python">batch_get_value(xs)
</code></pre>

<p>Returns the value of more than one tensor variable,
as a list of Numpy arrays.</p>
<hr />
<h3 id="print_tensor">print_tensor</h3>
<pre><code class="python">print_tensor(x, message='')
</code></pre>

<p>Print the message and the tensor when evaluated and return the same
tensor.</p>
<hr />
<h3 id="stop_gradient">stop_gradient</h3>
<pre><code class="python">stop_gradient(variables)
</code></pre>

<p>Returns <code>variables</code> but with zero gradient with respect to every other
variables.</p>
<hr />
<h3 id="rnn">rnn</h3>
<pre><code class="python">rnn(step_function, inputs, initial_states, go_backwards=False, mask=None, constants=None, unroll=False, input_length=None)
</code></pre>

<p>Iterates over the time dimension of a tensor.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>inputs</strong>: tensor of temporal data of shape (samples, time, ...)
(at least 3D).</li>
<li><strong>step_function</strong>:</li>
<li><strong>Parameters</strong>:<ul>
<li><strong>input</strong>: tensor with shape (samples, ...) (no time dimension),
representing input for the batch of samples at a certain
time step.</li>
<li><strong>states</strong>: list of tensors.</li>
</ul>
</li>
<li><strong>Returns</strong>:<ul>
<li><strong>output</strong>: tensor with shape (samples, ...) (no time dimension),</li>
<li><strong>new_states</strong>: list of tensors, same length and shapes
as 'states'.</li>
</ul>
</li>
<li><strong>initial_states</strong>: tensor with shape (samples, ...) (no time dimension),
containing the initial values for the states used in
the step function.</li>
<li><strong>go_backwards</strong>: boolean. If True, do the iteration over
the time dimension in reverse order.</li>
<li><strong>mask</strong>: binary tensor with shape (samples, time),
with a zero for every element that is masked.</li>
<li><strong>constants</strong>: a list of constant values passed at each step.</li>
<li><strong>unroll</strong>: whether to unroll the RNN or to use a symbolic loop (<code>scan</code>).</li>
<li><strong>input_length</strong>: must be specified if using <code>unroll</code>.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tuple (last_output, outputs, new_states).
- <strong>last_output</strong>: the latest output of the rnn, of shape (samples, ...)
- <strong>outputs</strong>: tensor with shape (samples, time, ...) where each
    entry outputs[s, t] is the output of the step function
    at time t for sample s.
- <strong>new_states</strong>: list of tensors, latest states returned by
    the step function, of shape (samples, ...).</p>
<hr />
<h3 id="switch">switch</h3>
<pre><code class="python">switch(condition, then_expression, else_expression)
</code></pre>

<p>condition: scalar tensor.</p>
<hr />
<h3 id="dropout">dropout</h3>
<pre><code class="python">dropout(x, level, noise_shape=None, seed=None)
</code></pre>

<p>Sets entries in <code>x</code> to zero at random,
while scaling the entire tensor.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: tensor</li>
<li><strong>level</strong>: fraction of the entries in the tensor
that will be set to 0.</li>
<li><strong>noise_shape</strong>: shape for randomly generated keep/drop flags,
must be broadcastable to the shape of <code>x</code></li>
<li><strong>seed</strong>: random seed to ensure determinism.</li>
</ul>
<hr />
<h3 id="conv2d">conv2d</h3>
<pre><code class="python">conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='th', image_shape=None, filter_shape=None, filter_dilation=(1, 1))
</code></pre>

<p>2D convolution.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>kernel</strong>: kernel tensor.</li>
<li><strong>strides</strong>: strides tuple.</li>
<li><strong>border_mode</strong>: string, "same" or "valid".</li>
<li><strong>dim_ordering</strong>: "tf" or "th".
Whether to use Theano or TensorFlow dimension ordering
in inputs/kernels/ouputs.</li>
</ul>
<hr />
<h3 id="deconv2d">deconv2d</h3>
<pre><code class="python">deconv2d(x, kernel, output_shape, strides=(1, 1), border_mode='valid', dim_ordering='th', image_shape=None, filter_shape=None)
</code></pre>

<p>2D deconvolution (transposed convolution).</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>kernel</strong>: kernel tensor.</li>
<li><strong>output_shape</strong>: desired dimensions of output.</li>
<li><strong>strides</strong>: strides tuple.</li>
<li><strong>border_mode</strong>: string, "same" or "valid".</li>
<li><strong>dim_ordering</strong>: "tf" or "th".
Whether to use Theano or TensorFlow dimension ordering
in inputs/kernels/ouputs.</li>
</ul>
<hr />
<h3 id="conv3d">conv3d</h3>
<pre><code class="python">conv3d(x, kernel, strides=(1, 1, 1), border_mode='valid', dim_ordering='th', volume_shape=None, filter_shape=None)
</code></pre>

<p>Run on cuDNN if available.
- <strong>border_mode</strong>: string, "same" or "valid".</p>
<hr />
<h3 id="ctc_batch_cost">ctc_batch_cost</h3>
<pre><code class="python">ctc_batch_cost(y_true, y_pred, input_length, label_length)
</code></pre>

<p>Runs CTC loss algorithm on each batch element.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>y_true</strong>: tensor (samples, max_string_length) containing the truth labels</li>
<li><strong>y_pred</strong>: tensor (samples, time_steps, num_categories) containing the prediction,
    or output of the softmax</li>
<li><strong>input_length</strong>: tensor (samples,1) containing the sequence length for
    each batch item in y_pred</li>
<li><strong>label_length</strong>: tensor (samples,1) containing the sequence length for
    each batch item in y_true</li>
</ul>
<p><strong>Returns</strong></p>
<p>Tensor with shape (samples,1) containing the
CTC loss of each element</p>
<hr />
<h3 id="backend">backend</h3>
<pre><code class="python">backend()
</code></pre>

<p>Publicly accessible method
for determining the current backend.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../initializations/" class="btn btn-neutral float-right" title="Initializations">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../applications/" class="btn btn-neutral" title="Applications"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="http://github.com/fchollet/keras" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../applications/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../initializations/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../js/theme.js"></script>

</body>
</html>
